{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split \n",
    "from torchmetrics.classification import F1Score, Accuracy\n",
    "\n",
    "from transformers import BartForSequenceClassification, BartTokenizer, AdamW, BartConfig\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/VUA18/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+'train.tsv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv(DATA_PATH+'test.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The number of training samples is: {len(train_df)} and the number of test samples is: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>w_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1g-fragment02 841</td>\n",
       "      <td>0</td>\n",
       "      <td>If it now seems self-evident that monitoring o...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fef-fragment03 667</td>\n",
       "      <td>0</td>\n",
       "      <td>Which equation should we use in a practical ca...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as6-fragment01 76</td>\n",
       "      <td>0</td>\n",
       "      <td>It was initiated partly in response to the fur...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ew1-fragment01 108</td>\n",
       "      <td>1</td>\n",
       "      <td>You fully know as an old pressman the difficul...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fpb-fragment01 1152</td>\n",
       "      <td>0</td>\n",
       "      <td>It was a condition of her gift to you the ten ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index  label  \\\n",
       "0   b1g-fragment02 841      0   \n",
       "1   fef-fragment03 667      0   \n",
       "2    as6-fragment01 76      0   \n",
       "3   ew1-fragment01 108      1   \n",
       "4  fpb-fragment01 1152      0   \n",
       "\n",
       "                                            sentence   POS  w_index  \n",
       "0  If it now seems self-evident that monitoring o...   ADP       42  \n",
       "1  Which equation should we use in a practical ca...  NOUN       10  \n",
       "2  It was initiated partly in response to the fur...   ADP       10  \n",
       "3  You fully know as an old pressman the difficul...  VERB       10  \n",
       "4  It was a condition of her gift to you the ten ...   ADJ        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    85177\n",
       "1    12481\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming you have access to the training dataset\n",
    "# Replace `train_dataset` with your actual training dataset\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32) # Convert to tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5733, 3.9123])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sentence = self.data.iloc[idx]['sentence']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        pos_tag = self.data.iloc[idx]['POS']\n",
    "\n",
    "        sentence_encoding = self.tokenizer(sentence, \n",
    "                                           truncation = True, \n",
    "                                           padding='max_length', \n",
    "                                           max_length=self.max_length,\n",
    "                                           return_tensors= 'pt')\n",
    "        # pos_encoding = self.tokenizer(pos_tag,\n",
    "        #                               truncation = True,\n",
    "        #                               padding = 'max_length',\n",
    "        #                               max_length = 5,\n",
    "        #                               return_tensors= 'pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': sentence_encoding['input_ids'],\n",
    "            'attention_mask': sentence_encoding['attention_mask'],\n",
    "            # 'pos_ids': pos_encoding['input_ids'],\n",
    "            'label': torch.tensor(label, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBartForSequenceClassification(BartForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classification_head = nn.Linear(config.d_model, 2)  # Add your custom classification head\n",
    "\n",
    "config = BartConfig.from_pretrained('facebook/bart-large')\n",
    "model = CustomBartForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/bart-large-mnli\"\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "# model = BartForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # You can adjust this based on your dataset\n",
    "batch_size = 32   # You can adjust this as well\n",
    "train_dataset = MetaphorDataset(train_df, tokenizer, max_length)\n",
    "test_dataset = MetaphorDataset(test_df, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples in train set: 78127\n",
      "samples in test set: 43947\n",
      "samples in val set: 19531\n"
     ]
    }
   ],
   "source": [
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = int(len(train_dataset)- val_size)\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"samples in train set: {len(train_set)}\")\n",
    "print(f\"samples in test set: {len(test_dataset)}\")\n",
    "print(f\"samples in val set: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([32, 1, 128])\n",
      "Attention Mask: torch.Size([32, 1, 128])\n",
      "Labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    # pos_ids = batch['pos_ids']\n",
    "    labels = batch['label']\n",
    "\n",
    "    print(\"Input IDs:\", input_ids.shape)\n",
    "    print(\"Attention Mask:\", attention_mask.shape)\n",
    "    # print(\"POS IDs:\", pos_ids.shape)\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr' : 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, model_name='facebook/bart-base', config=config, class_weights = None):\n",
    "        super(MetaphorClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "        # Load pre-trained BART model and tokenizer\n",
    "        self.bart = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bart(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "    def common_step(self, batch, step_type: str):\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        labels = batch['label']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        device = logits.device\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss(weight = self.class_weights.to(device))\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        accuracy = Accuracy(task='binary').to(device)\n",
    "\n",
    "        self.log(f'{step_type}_loss', loss)\n",
    "        self.log(f'{step_type}_acc', accuracy(predictions, labels))\n",
    "        return loss, logits, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='val')\n",
    "        # device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(logits.device)\n",
    "        labels = labels.to(logits.device)\n",
    "\n",
    "        f1 = F1Score(task='binary').to(logits.device)\n",
    "        self.log('F1_score', f1(predictions, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='test')\n",
    "        predictions = torch.argmax(logits, dim=1).to(logits.device)\n",
    "        labels = labels.to(logits.device)\n",
    "\n",
    "        accuracy = Accuracy().to(logits.device)\n",
    "        f1 = F1Score(task='binary').to(logits.device)\n",
    "\n",
    "        self.log('test_acc', accuracy(logits, labels))\n",
    "        self.log('test_f1', f1(predictions, labels))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.config['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilbert(config, num_epochs = 8, num_classes=2, checkpoint = None):\n",
    "\n",
    "    model = MetaphorClassifier(config = config , num_classes = num_classes, class_weights=class_weights)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"metaphor-logs\", name =\"bart-full\", version = 'v1')\n",
    "\n",
    "    db_callbacks = [\n",
    "        callbacks.ModelCheckpoint(monitor= 'val_loss',\n",
    "                                  save_top_k = 1,\n",
    "                                   save_on_train_epoch_end= False,\n",
    "                                    filename = '{epoch}-{val_loss:.2f}' )\n",
    "\n",
    "    ]\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps =2,\n",
    "                         precision = 16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= db_callbacks,\n",
    "                         devices = 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs)\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=checkpoint)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory metaphor-logs/bart-full/v1/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | bart | BartForSequenceClassification | 140 M \n",
      "-------------------------------------------------------\n",
      "140 M     Trainable params\n",
      "0         Non-trainable params\n",
      "140 M     Total params\n",
      "560.050   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2442/2442 [11:52<00:00,  3.43it/s, v_num=v1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2442/2442 [11:52<00:00,  3.43it/s, v_num=v1]\n"
     ]
    }
   ],
   "source": [
    "db_trainer = train_distilbert(config= config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e58d43a12d7d4eda\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e58d43a12d7d4eda\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8081;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"/home/vri/Projects/research/metaphor-detection/notebooks/metaphor-logs\" --host localhost --port 8081\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at metaphor-logs/bart-full/v1/checkpoints/epoch=0-val_loss=0.62.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at metaphor-logs/bart-full/v1/checkpoints/epoch=0-val_loss=0.62.ckpt\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 1374/1374 [01:54<00:00, 12.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         F1_score          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2079881876707077     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5613579750061035     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6664794087409973     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        F1_score         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2079881876707077    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5613579750061035    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6664794087409973    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.6664794087409973,\n",
       "  'val_acc': 0.5613579750061035,\n",
       "  'F1_score': 0.2079881876707077}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_trainer.validate(dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
