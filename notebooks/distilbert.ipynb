{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split \n",
    "from torchmetrics.classification import F1Score, Accuracy\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification, DistilBertConfig, AdamW, DistilBertModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/VUA18/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+'train.tsv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv(DATA_PATH+'test.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The number of training samples is: {len(train_df)} and the number of test samples is: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>w_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1g-fragment02 841</td>\n",
       "      <td>0</td>\n",
       "      <td>If it now seems self-evident that monitoring o...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fef-fragment03 667</td>\n",
       "      <td>0</td>\n",
       "      <td>Which equation should we use in a practical ca...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as6-fragment01 76</td>\n",
       "      <td>0</td>\n",
       "      <td>It was initiated partly in response to the fur...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ew1-fragment01 108</td>\n",
       "      <td>1</td>\n",
       "      <td>You fully know as an old pressman the difficul...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fpb-fragment01 1152</td>\n",
       "      <td>0</td>\n",
       "      <td>It was a condition of her gift to you the ten ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index  label  \\\n",
       "0   b1g-fragment02 841      0   \n",
       "1   fef-fragment03 667      0   \n",
       "2    as6-fragment01 76      0   \n",
       "3   ew1-fragment01 108      1   \n",
       "4  fpb-fragment01 1152      0   \n",
       "\n",
       "                                            sentence   POS  w_index  \n",
       "0  If it now seems self-evident that monitoring o...   ADP       42  \n",
       "1  Which equation should we use in a practical ca...  NOUN       10  \n",
       "2  It was initiated partly in response to the fur...   ADP       10  \n",
       "3  You fully know as an old pressman the difficul...  VERB       10  \n",
       "4  It was a condition of her gift to you the ten ...   ADJ        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sentence = self.data.iloc[idx]['sentence']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        pos_tag = self.data.iloc[idx]['POS']\n",
    "\n",
    "        sentence_encoding = self.tokenizer(sentence, \n",
    "                                           truncation = True, \n",
    "                                           padding='max_length', \n",
    "                                           max_length=self.max_length,\n",
    "                                           return_tensors= 'pt')\n",
    "        # pos_encoding = self.tokenizer(pos_tag,\n",
    "        #                               truncation = True,\n",
    "        #                               padding = 'max_length',\n",
    "        #                               max_length = 5,\n",
    "        #                               return_tensors= 'pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': sentence_encoding['input_ids'],\n",
    "            'attention_mask': sentence_encoding['attention_mask'],\n",
    "            # 'pos_ids': pos_encoding['input_ids'],\n",
    "            'label': torch.tensor(label, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # You can adjust this based on your dataset\n",
    "batch_size = 32   # You can adjust this as well\n",
    "train_dataset = MetaphorDataset(train_df, tokenizer, max_length)\n",
    "test_dataset = MetaphorDataset(test_df, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples in train set: 78127\n",
      "samples in test set: 43947\n",
      "samples in val set: 19531\n"
     ]
    }
   ],
   "source": [
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = int(len(train_dataset)- val_size)\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"samples in train set: {len(train_set)}\")\n",
    "print(f\"samples in test set: {len(test_dataset)}\")\n",
    "print(f\"samples in val set: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([32, 1, 128])\n",
      "Attention Mask: torch.Size([32, 1, 128])\n",
      "Labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    # pos_ids = batch['pos_ids']\n",
    "    labels = batch['label']\n",
    "\n",
    "    print(\"Input IDs:\", input_ids.shape)\n",
    "    print(\"Attention Mask:\", attention_mask.shape)\n",
    "    # print(\"POS IDs:\", pos_ids.shape)\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr' : 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, model_name = 'distilbert-base-uncased', config= config) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config \n",
    "        self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # print(self.model)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        # pos_ids = batch['pos_ids']\n",
    "        sen_outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        pooled_sen_output = sen_outputs['last_hidden_state'][:, 0]\n",
    "\n",
    "        # pos_outputs = self.model(input_ids = pos_ids)[0]\n",
    "        # pooled_pos_output = pos_outputs[:, 0]\n",
    "\n",
    "        # combined_output = torch.cat((pooled_sen_output, pooled_pos_output), dim=1)\n",
    "\n",
    "        # if pos_ids is not None:\n",
    "        #     pos_outputs = self.model(input_ids=pos_ids)[0]\n",
    "        #     pooled_pos_output = pos_outputs[:, 0]\n",
    "        #     combined_output = torch.cat((pooled_sen_output, pooled_pos_output), dim=1)\n",
    "        # else:\n",
    "        #     combined_output = pooled_sen_output\n",
    "\n",
    "        logits = self.classifier(pooled_sen_output)\n",
    "        return logits\n",
    "    \n",
    "    def common_step(self, batch, step_type:str):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        # pos_ids = batch['pos_ids']\n",
    "\n",
    "        data = {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask}\n",
    "                # 'pos_ids' : pos_ids}\n",
    "\n",
    "        labels = batch['label']\n",
    "        logits = self.forward(data)\n",
    "\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        accuracy = Accuracy(task='binary').to(device)\n",
    "        \n",
    "        self.log(f'{step_type}_loss', loss)\n",
    "        self.log(f'{step_type}_acc', accuracy(predictions, labels))\n",
    "        return loss, logits, labels\n",
    "    \n",
    "    def accuracy(self, correct, labels):\n",
    "        return correct/len(labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='train')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='val')\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        f1=F1Score(task='binary').to(device)\n",
    "\n",
    "        self.log('F1_score', f1(predictions, labels))\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        data = {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask}\n",
    "                # 'pos_ids' : pos_ids}\n",
    "\n",
    "        # labels = batch['label']\n",
    "        logits = self.forward(data)\n",
    "\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        accuracy = Accuracy(task='binary').to(device)\n",
    "        f1=F1Score(task='binary').to(device)\n",
    "\n",
    "        self.log('test_acc', accuracy(predictions, labels))\n",
    "        self.log('test_f1', predictions, labels)\n",
    "\n",
    "        # accuracy = Accuracy()(logits, labels)\n",
    "        # f1 = F1Score(logits, labels)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr = self.config['lr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilbert(config, num_epochs = 8, num_classes=2, checkpoint = None):\n",
    "\n",
    "    model = MetaphorClassifier(config = config , num_classes = num_classes)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"metaphor-logs\", name =\"distilbert-full\", version = 'v1')\n",
    "\n",
    "    db_callbacks = [\n",
    "        callbacks.ModelCheckpoint(monitor= 'val_loss',\n",
    "                                  save_top_k = 1,\n",
    "                                   save_on_train_epoch_end= False,\n",
    "                                    filename = '{epoch}-{val_loss:.2f}' )\n",
    "\n",
    "    ]\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps =2,\n",
    "                         precision = 16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= db_callbacks,\n",
    "                         devices = 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs)\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=checkpoint)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory metaphor-logs/distilbert-full/v1/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | model      | DistilBertModel | 66.4 M\n",
      "1 | classifier | Linear          | 1.5 K \n",
      "-----------------------------------------------\n",
      "66.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.4 M    Total params\n",
      "265.458   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 21.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2442/2442 [05:07<00:00,  7.95it/s, v_num=v1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2442/2442 [05:07<00:00,  7.95it/s, v_num=v1]\n"
     ]
    }
   ],
   "source": [
    "db_trainer = train_distilbert(config= config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8075f95b9c6d39cf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8075f95b9c6d39cf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8081;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"/home/vri/Projects/research/metaphor-detection/notebooks/metaphor-logs\" --host localhost --port 8081\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at metaphor-logs/distilbert-full/v1/checkpoints/epoch=0-val_loss=0.36-v2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at metaphor-logs/distilbert-full/v1/checkpoints/epoch=0-val_loss=0.36-v2.ckpt\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 1374/1374 [01:00<00:00, 22.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         F1_score          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.002378622768446803    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8581245541572571     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3958614766597748     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        F1_score         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.002378622768446803   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8581245541572571    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3958614766597748    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.3958614766597748,\n",
       "  'val_acc': 0.8581245541572571,\n",
       "  'F1_score': 0.002378622768446803}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_trainer.validate(dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
