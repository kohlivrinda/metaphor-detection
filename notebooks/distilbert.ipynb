{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split \n",
    "from torchmetrics.classification import F1Score, Accuracy\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification, DistilBertConfig, AdamW, DistilBertModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/VUA18/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+'train.tsv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv(DATA_PATH+'test.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The number of training samples is: {len(train_df)} and the number of test samples is: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>w_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1g-fragment02 841</td>\n",
       "      <td>0</td>\n",
       "      <td>If it now seems self-evident that monitoring o...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fef-fragment03 667</td>\n",
       "      <td>0</td>\n",
       "      <td>Which equation should we use in a practical ca...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as6-fragment01 76</td>\n",
       "      <td>0</td>\n",
       "      <td>It was initiated partly in response to the fur...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ew1-fragment01 108</td>\n",
       "      <td>1</td>\n",
       "      <td>You fully know as an old pressman the difficul...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fpb-fragment01 1152</td>\n",
       "      <td>0</td>\n",
       "      <td>It was a condition of her gift to you the ten ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index  label  \\\n",
       "0   b1g-fragment02 841      0   \n",
       "1   fef-fragment03 667      0   \n",
       "2    as6-fragment01 76      0   \n",
       "3   ew1-fragment01 108      1   \n",
       "4  fpb-fragment01 1152      0   \n",
       "\n",
       "                                            sentence   POS  w_index  \n",
       "0  If it now seems self-evident that monitoring o...   ADP       42  \n",
       "1  Which equation should we use in a practical ca...  NOUN       10  \n",
       "2  It was initiated partly in response to the fur...   ADP       10  \n",
       "3  You fully know as an old pressman the difficul...  VERB       10  \n",
       "4  It was a condition of her gift to you the ten ...   ADJ        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sentence = self.data.iloc[idx]['sentence']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        # pos_tag = self.data.iloc[idx]['POS']\n",
    "\n",
    "        sentence_encoding = self.tokenizer(sentence, \n",
    "                                           truncation = True, \n",
    "                                           padding='max_length', \n",
    "                                           max_length=self.max_length,\n",
    "                                           return_tensors= 'pt')\n",
    "        # pos_encoding = self.tokenizer(pos_tag,\n",
    "        #                               truncation = True,\n",
    "        #                               padding = 'max_length',\n",
    "        #                               max_length = 5,\n",
    "        #                               return_tensors= 'pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': sentence_encoding['input_ids'],\n",
    "            'attention_mask': sentence_encoding['attention_mask'],\n",
    "            # 'pos_ids': pos_encoding['input_ids'],\n",
    "            'label': torch.tensor(label, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # You can adjust this based on your dataset\n",
    "batch_size = 32   # You can adjust this as well\n",
    "train_dataset = MetaphorDataset(train_df, tokenizer, max_length)\n",
    "test_dataset = MetaphorDataset(test_df, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples in train set: 78127\n",
      "samples in test set: 43947\n",
      "samples in val set: 19531\n"
     ]
    }
   ],
   "source": [
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = int(len(train_dataset)- val_size)\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"samples in train set: {len(train_set)}\")\n",
    "print(f\"samples in test set: {len(test_dataset)}\")\n",
    "print(f\"samples in val set: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([32, 1, 128])\n",
      "Attention Mask: torch.Size([32, 1, 128])\n",
      "Labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    # pos_ids = batch['pos_ids']\n",
    "    labels = batch['label']\n",
    "\n",
    "    print(\"Input IDs:\", input_ids.shape)\n",
    "    print(\"Attention Mask:\", attention_mask.shape)\n",
    "    # print(\"POS IDs:\", pos_ids.shape)\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr' : 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, model_name = 'distilbert-base-uncased', config= config) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config \n",
    "        self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # print(self.model)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        # pos_ids = batch['pos_ids']\n",
    "        sen_outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        pooled_sen_output = sen_outputs['last_hidden_state'][:, 0]\n",
    "\n",
    "        # pos_outputs = self.model(input_ids = pos_ids)[0]\n",
    "        # pooled_pos_output = pos_outputs[:, 0]\n",
    "\n",
    "        # combined_output = torch.cat((pooled_sen_output, pooled_pos_output), dim=1)\n",
    "\n",
    "        # if pos_ids is not None:\n",
    "        #     pos_outputs = self.model(input_ids=pos_ids)[0]\n",
    "        #     pooled_pos_output = pos_outputs[:, 0]\n",
    "        #     combined_output = torch.cat((pooled_sen_output, pooled_pos_output), dim=1)\n",
    "        # else:\n",
    "        #     combined_output = pooled_sen_output\n",
    "\n",
    "        logits = self.classifier(pooled_sen_output)\n",
    "        return logits\n",
    "    \n",
    "    def common_step(self, batch, step_type:str):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        # pos_ids = batch['pos_ids']\n",
    "\n",
    "        data = {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask}\n",
    "                # 'pos_ids' : pos_ids}\n",
    "\n",
    "        labels = batch['label']\n",
    "        logits = self.forward(data)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        # accuracy = Accuracy(task='binary')  # No need to specify the number of classes for binary classification\n",
    "        # acc = accuracy(logits, labels)\n",
    "        \n",
    "        self.log(f'{step_type}_loss', loss)\n",
    "        # self.log(f'{step_type}_acc', acc(logits, labels))\n",
    "        return loss, logits, labels\n",
    "    \n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     input_ids = batch['input_ids']\n",
    "    #     attention_mask = batch['attention_mask']\n",
    "    #     labels = batch['label']\n",
    "    #     logits = self(input_ids, attention_mask)\n",
    "    #     loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "    #     self.log('train_loss', loss)\n",
    "    #     return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='train')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='val')\n",
    "        # self.log('F1_score', F1Score(logits, labels))\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        # accuracy = Accuracy()(logits, labels)\n",
    "        # f1 = F1Score(logits, labels)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr = self.config['lr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilbert(config, num_epochs = 8, num_classes=2, checkpoint = None):\n",
    "\n",
    "    model = MetaphorClassifier(config = config , num_classes = num_classes)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"metaphor-logs\", name =\"distilbert-full\", version = 'v1')\n",
    "\n",
    "    db_callbacks = [\n",
    "        callbacks.ModelCheckpoint(monitor= 'val_loss',\n",
    "                                  save_top_k = 1,\n",
    "                                   save_on_train_epoch_end= False,\n",
    "                                    filename = '{epoch}-{val_loss:.2f}' )\n",
    "\n",
    "    ]\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps =2,\n",
    "                         precision = 16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= db_callbacks,\n",
    "                         devices = 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs)\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=checkpoint)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | model      | DistilBertModel | 66.4 M\n",
      "1 | classifier | Linear          | 1.5 K \n",
      "-----------------------------------------------\n",
      "66.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.4 M    Total params\n",
      "265.458   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   1%|‚ñè         | 36/2442 [00:05<06:24,  6.26it/s, v_num=v1]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "db_trainer = train_distilbert(config= config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cfe8a87600d3dbfc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cfe8a87600d3dbfc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8081;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"notebooks/metaphor-logs\" --host localhost --port 8081\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
