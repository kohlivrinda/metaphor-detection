{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split \n",
    "from torchmetrics.classification import F1Score, Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification, DistilBertConfig, AdamW, DistilBertModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/VUA18/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+'train.tsv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv(DATA_PATH+'test.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The number of training samples is: {len(train_df)} and the number of test samples is: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>w_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1g-fragment02 841</td>\n",
       "      <td>0</td>\n",
       "      <td>If it now seems self-evident that monitoring o...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fef-fragment03 667</td>\n",
       "      <td>0</td>\n",
       "      <td>Which equation should we use in a practical ca...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as6-fragment01 76</td>\n",
       "      <td>0</td>\n",
       "      <td>It was initiated partly in response to the fur...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ew1-fragment01 108</td>\n",
       "      <td>1</td>\n",
       "      <td>You fully know as an old pressman the difficul...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fpb-fragment01 1152</td>\n",
       "      <td>0</td>\n",
       "      <td>It was a condition of her gift to you the ten ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index  label  \\\n",
       "0   b1g-fragment02 841      0   \n",
       "1   fef-fragment03 667      0   \n",
       "2    as6-fragment01 76      0   \n",
       "3   ew1-fragment01 108      1   \n",
       "4  fpb-fragment01 1152      0   \n",
       "\n",
       "                                            sentence   POS  w_index  \n",
       "0  If it now seems self-evident that monitoring o...   ADP       42  \n",
       "1  Which equation should we use in a practical ca...  NOUN       10  \n",
       "2  It was initiated partly in response to the fur...   ADP       10  \n",
       "3  You fully know as an old pressman the difficul...  VERB       10  \n",
       "4  It was a condition of her gift to you the ten ...   ADJ        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>w_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25312</th>\n",
       "      <td>as6-fragment01 79</td>\n",
       "      <td>0</td>\n",
       "      <td>The scope was extended in an enhanced Urban Pr...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65590</th>\n",
       "      <td>fet-fragment01 197</td>\n",
       "      <td>0</td>\n",
       "      <td>In the churchyard in his last parish a family ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23269</th>\n",
       "      <td>as6-fragment01 72</td>\n",
       "      <td>1</td>\n",
       "      <td>The main thrust of the Government's policy is ...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87309</th>\n",
       "      <td>kcu-fragment02 2108</td>\n",
       "      <td>0</td>\n",
       "      <td>come home from work she said I've been looking...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73907</th>\n",
       "      <td>kbj-fragment17 1470</td>\n",
       "      <td>1</td>\n",
       "      <td>Those two add?</td>\n",
       "      <td>VERB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69075</th>\n",
       "      <td>ac2-fragment06 1482</td>\n",
       "      <td>0</td>\n",
       "      <td>We're not gon na change anyone's mind.</td>\n",
       "      <td>ADV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30130</th>\n",
       "      <td>fet-fragment01 36</td>\n",
       "      <td>0</td>\n",
       "      <td>There is still a great deal of Greece all thro...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80941</th>\n",
       "      <td>a1h-fragment06 144</td>\n",
       "      <td>0</td>\n",
       "      <td>It was a blessing that, in response to congrat...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41098</th>\n",
       "      <td>b1g-fragment02 798</td>\n",
       "      <td>0</td>\n",
       "      <td>This contribution cuts across many of the ques...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86154</th>\n",
       "      <td>fet-fragment01 167</td>\n",
       "      <td>0</td>\n",
       "      <td>Frederica and Alexander held a discussion of n...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78126 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index  label  \\\n",
       "25312    as6-fragment01 79      0   \n",
       "65590   fet-fragment01 197      0   \n",
       "23269    as6-fragment01 72      1   \n",
       "87309  kcu-fragment02 2108      0   \n",
       "73907  kbj-fragment17 1470      1   \n",
       "...                    ...    ...   \n",
       "69075  ac2-fragment06 1482      0   \n",
       "30130    fet-fragment01 36      0   \n",
       "80941   a1h-fragment06 144      0   \n",
       "41098   b1g-fragment02 798      0   \n",
       "86154   fet-fragment01 167      0   \n",
       "\n",
       "                                                sentence   POS  w_index  \n",
       "25312  The scope was extended in an enhanced Urban Pr...  VERB        2  \n",
       "65590  In the churchyard in his last parish a family ...   ADJ        5  \n",
       "23269  The main thrust of the Government's policy is ...  NOUN       46  \n",
       "87309  come home from work she said I've been looking...  VERB       24  \n",
       "73907                                     Those two add?  VERB        2  \n",
       "...                                                  ...   ...      ...  \n",
       "69075             We're not gon na change anyone's mind.   ADV        1  \n",
       "30130  There is still a great deal of Greece all thro...  NOUN       22  \n",
       "80941  It was a blessing that, in response to congrat...  INTJ       27  \n",
       "41098  This contribution cuts across many of the ques...  NOUN        7  \n",
       "86154  Frederica and Alexander held a discussion of n...  NOUN        8  \n",
       "\n",
       "[78126 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>w_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91201</th>\n",
       "      <td>a3e-fragment03 35</td>\n",
       "      <td>0</td>\n",
       "      <td>You may, even as a novice, care so passionatel...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80346</th>\n",
       "      <td>kb7-fragment10 2284</td>\n",
       "      <td>0</td>\n",
       "      <td>I mean we've just been to look at some others ...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36265</th>\n",
       "      <td>cdb-fragment04 945</td>\n",
       "      <td>0</td>\n",
       "      <td>In high spirits, his father was talking about ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76505</th>\n",
       "      <td>c8t-fragment01 95</td>\n",
       "      <td>0</td>\n",
       "      <td>She said briefly: They know me.</td>\n",
       "      <td>ADV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19291</th>\n",
       "      <td>a6u-fragment02 294</td>\n",
       "      <td>0</td>\n",
       "      <td>The physicality so characteristic of Kahlo's w...</td>\n",
       "      <td>DET</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43437</th>\n",
       "      <td>fet-fragment01 38</td>\n",
       "      <td>0</td>\n",
       "      <td>The youth of it, Alexander thought.</td>\n",
       "      <td>ADP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42038</th>\n",
       "      <td>a1j-fragment34 558</td>\n",
       "      <td>0</td>\n",
       "      <td>There could also be controversy over the execu...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94551</th>\n",
       "      <td>a6u-fragment02 274</td>\n",
       "      <td>0</td>\n",
       "      <td>This small painting on metal, in the style of ...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89914</th>\n",
       "      <td>g0l-fragment01 68</td>\n",
       "      <td>0</td>\n",
       "      <td>Our information continued for a while, but it ...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22589</th>\n",
       "      <td>as6-fragment01 28</td>\n",
       "      <td>0</td>\n",
       "      <td>Harrison describes( p. 369) the way in which t...</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19532 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index  label  \\\n",
       "91201    a3e-fragment03 35      0   \n",
       "80346  kb7-fragment10 2284      0   \n",
       "36265   cdb-fragment04 945      0   \n",
       "76505    c8t-fragment01 95      0   \n",
       "19291   a6u-fragment02 294      0   \n",
       "...                    ...    ...   \n",
       "43437    fet-fragment01 38      0   \n",
       "42038   a1j-fragment34 558      0   \n",
       "94551   a6u-fragment02 274      0   \n",
       "89914    g0l-fragment01 68      0   \n",
       "22589    as6-fragment01 28      0   \n",
       "\n",
       "                                                sentence   POS  w_index  \n",
       "91201  You may, even as a novice, care so passionatel...   ADJ       17  \n",
       "80346  I mean we've just been to look at some others ...  NOUN        9  \n",
       "36265  In high spirits, his father was talking about ...   ADJ       16  \n",
       "76505                    She said briefly: They know me.   ADV        2  \n",
       "19291  The physicality so characteristic of Kahlo's w...   DET       17  \n",
       "...                                                  ...   ...      ...  \n",
       "43437                The youth of it, Alexander thought.   ADP        2  \n",
       "42038  There could also be controversy over the execu...   ADP       16  \n",
       "94551  This small painting on metal, in the style of ...   ADJ       19  \n",
       "89914  Our information continued for a while, but it ...  NOUN        1  \n",
       "22589  Harrison describes( p. 369) the way in which t...   ADJ       18  \n",
       "\n",
       "[19532 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_df['label'])\n",
    "new_weights = torch.tensor(class_weights, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sentence = self.data.iloc[idx]['sentence']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        pos_tag = self.data.iloc[idx]['POS']\n",
    "\n",
    "        sentence_encoding = self.tokenizer(sentence, \n",
    "                                           truncation = True, \n",
    "                                           padding='max_length', \n",
    "                                           max_length=self.max_length,\n",
    "                                           return_tensors= 'pt')\n",
    "        # pos_encoding = self.tokenizer(pos_tag,\n",
    "        #                               truncation = True,\n",
    "        #                               padding = 'max_length',\n",
    "        #                               max_length = 5,\n",
    "        #                               return_tensors= 'pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': sentence_encoding['input_ids'],\n",
    "            'attention_mask': sentence_encoding['attention_mask'],\n",
    "            # 'pos_ids': pos_encoding['input_ids'],\n",
    "            'label': torch.tensor(label, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # You can adjust this based on your dataset\n",
    "batch_size = 32   # You can adjust this as well\n",
    "train_dataset = MetaphorDataset(train_df, tokenizer, max_length)\n",
    "val_dataset = MetaphorDataset(val_df, tokenizer, max_length)\n",
    "test_dataset = MetaphorDataset(test_df, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples in train set: 78126\n",
      "samples in test set: 43947\n",
      "samples in val set: 19532\n"
     ]
    }
   ],
   "source": [
    "# val_size = int(0.2 * len(train_dataset))\n",
    "# train_size = int(len(train_dataset)- val_size)\n",
    "# train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"samples in train set: {len(train_dataset)}\")\n",
    "print(f\"samples in test set: {len(test_dataset)}\")\n",
    "print(f\"samples in val set: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([32, 1, 128])\n",
      "Attention Mask: torch.Size([32, 1, 128])\n",
      "Labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    # pos_ids = batch['pos_ids']\n",
    "    labels = batch['label']\n",
    "\n",
    "    print(\"Input IDs:\", input_ids.shape)\n",
    "    print(\"Attention Mask:\", attention_mask.shape)\n",
    "    # print(\"POS IDs:\", pos_ids.shape)\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr' : 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, model_name = 'distilbert-base-uncased', config= config, class_weights = None) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config \n",
    "        self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        self.weights = class_weights\n",
    "        hidden_size = self.model.config.hidden_size \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # print(self.model)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        # pos_ids = batch['pos_ids']\n",
    "        sen_outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        pooled_sen_output = sen_outputs['last_hidden_state'][:, 0]\n",
    "\n",
    "        # pos_outputs = self.model(input_ids = pos_ids)[0]\n",
    "        # pooled_pos_output = pos_outputs[:, 0]\n",
    "\n",
    "        # combined_output = torch.cat((pooled_sen_output, pooled_pos_output), dim=1)\n",
    "\n",
    "        # if pos_ids is not None:\n",
    "        #     pos_outputs = self.model(input_ids=pos_ids)[0]\n",
    "        #     pooled_pos_output = pos_outputs[:, 0]\n",
    "        #     combined_output = torch.cat((pooled_sen_output, pooled_pos_output), dim=1)\n",
    "        # else:\n",
    "        #     combined_output = pooled_sen_output\n",
    "\n",
    "        logits = self.classifier(pooled_sen_output)\n",
    "        return logits\n",
    "    \n",
    "    def common_step(self, batch, step_type:str):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        # pos_ids = batch['pos_ids']\n",
    "\n",
    "        data = {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask}\n",
    "                # 'pos_ids' : pos_ids}\n",
    "\n",
    "        labels = batch['label']\n",
    "        logits = self.forward(data)\n",
    "\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        loss = nn.CrossEntropyLoss(weight=self.weights.to(device))(logits, labels)\n",
    "\n",
    "        accuracy = Accuracy(task='binary').to(device)\n",
    "        \n",
    "        self.log(f'{step_type}_loss', loss)\n",
    "        self.log(f'{step_type}_acc', accuracy(predictions, labels))\n",
    "        return loss, logits, labels\n",
    "    \n",
    "    def accuracy(self, correct, labels):\n",
    "        return correct/len(labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='train')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self.common_step(batch, step_type='val')\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        f1=F1Score(task='binary').to(device)\n",
    "\n",
    "        self.log('F1_score', f1(predictions, labels))\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        data = {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask}\n",
    "                # 'pos_ids' : pos_ids}\n",
    "\n",
    "        # labels = batch['label']\n",
    "        logits = self.forward(data)\n",
    "\n",
    "        device = logits.device\n",
    "        predictions = torch.argmax(logits, dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        accuracy = Accuracy(task='binary').to(device)\n",
    "        f1=F1Score(task='binary').to(device)\n",
    "\n",
    "        self.log('test_acc', accuracy(predictions, labels))\n",
    "        self.log('test_f1', predictions, labels)\n",
    "\n",
    "        # accuracy = Accuracy()(logits, labels)\n",
    "        # f1 = F1Score(logits, labels)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr = self.config['lr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilbert(config, num_epochs = 8, num_classes=2, checkpoint = None):\n",
    "\n",
    "    model = MetaphorClassifier(config = config , num_classes = num_classes, class_weights=new_weights)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"metaphor-logs\", name =\"distilbert-weighted\", version = 'v1')\n",
    "\n",
    "    db_callbacks = [\n",
    "        callbacks.ModelCheckpoint(monitor= 'val_loss',\n",
    "                                  save_top_k = 1,\n",
    "                                   save_on_train_epoch_end= False,\n",
    "                                    filename = '{epoch}-{val_loss:.2f}' )\n",
    "\n",
    "    ]\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps =2,\n",
    "                         precision = 16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= db_callbacks,\n",
    "                         devices = 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs)\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=checkpoint)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | model      | DistilBertModel | 66.4 M\n",
      "1 | classifier | Linear          | 1.5 K \n",
      "-----------------------------------------------\n",
      "66.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.4 M    Total params\n",
      "265.458   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2442/2442 [05:03<00:00,  8.06it/s, v_num=v1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2442/2442 [05:03<00:00,  8.06it/s, v_num=v1]\n"
     ]
    }
   ],
   "source": [
    "db_trainer = train_distilbert(config= config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"/home/vri/Projects/research/metaphor-detection/notebooks/metaphor-logs\" --host localhost --port 8081\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at metaphor-logs/distilbert-weighted/v1/checkpoints/epoch=0-val_loss=0.69.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at metaphor-logs/distilbert-weighted/v1/checkpoints/epoch=0-val_loss=0.69.ckpt\n",
      "/home/vri/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1374 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`self.log(test_f1, tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))` was called, but the tensor must have a single element. You can try doing `self.log(test_f1, tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0').mean())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vri/Projects/research/metaphor-detection/notebooks/distilbert.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vri/Projects/research/metaphor-detection/notebooks/distilbert.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m db_trainer\u001b[39m.\u001b[39;49mtest(dataloaders\u001b[39m=\u001b[39;49mtest_loader, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    704\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    707\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    708\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    746\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    748\u001b[0m )\n\u001b[0;32m--> 749\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    750\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    751\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:387\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtest_step_context():\n\u001b[1;32m    386\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/vri/Projects/research/metaphor-detection/notebooks/distilbert.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vri/Projects/research/metaphor-detection/notebooks/distilbert.ipynb#X23sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m f1\u001b[39m=\u001b[39mF1Score(task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vri/Projects/research/metaphor-detection/notebooks/distilbert.ipynb#X23sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m'\u001b[39m, accuracy(predictions, labels))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vri/Projects/research/metaphor-detection/notebooks/distilbert.ipynb#X23sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog(\u001b[39m'\u001b[39;49m\u001b[39mtest_f1\u001b[39;49m\u001b[39m'\u001b[39;49m, predictions, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/core/module.py:444\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/dataloader_idx_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name:\n\u001b[1;32m    439\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou called `self.log` with the key `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but it should not contain information about `dataloader_idx`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m     )\n\u001b[0;32m--> 444\u001b[0m value \u001b[39m=\u001b[39m apply_to_collection(value, (Tensor, numbers\u001b[39m.\u001b[39;49mNumber), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__to_tensor, name)\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mshould_reset_tensors(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_fx_name):\n\u001b[1;32m    447\u001b[0m     \u001b[39m# if we started a new epoch (running its first batch) the hook name has changed\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# reset any tensors for the new hook name\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     results\u001b[39m.\u001b[39mreset(metrics\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, fx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_fx_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py:51\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     53\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/core/module.py:617\u001b[0m, in \u001b[0;36mLightningModule.__to_tensor\u001b[0;34m(self, value, name)\u001b[0m\n\u001b[1;32m    611\u001b[0m value \u001b[39m=\u001b[39m (\n\u001b[1;32m    612\u001b[0m     value\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor)\n\u001b[1;32m    614\u001b[0m     \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    615\u001b[0m )\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mnumel(value) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    618\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`self.log(\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m)` was called, but the tensor must have a single element.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m You can try doing `self.log(\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m.mean())`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    621\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m    622\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: `self.log(test_f1, tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))` was called, but the tensor must have a single element. You can try doing `self.log(test_f1, tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0').mean())`"
     ]
    }
   ],
   "source": [
    "db_trainer.test(dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
